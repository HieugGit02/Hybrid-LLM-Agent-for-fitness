"""
=====================================================
COMPLETE NOTEBOOK CELL FOR COLAB (ALL-IN-ONE)
=====================================================
Ch·∫°y file n√†y ·ªü m·ªôt cell trong Google Colab ƒë·ªÉ c√≥ ƒë·ªß:
1. Data loading
2. Vector store setup  
3. LLM initialization
4. Flask API + Ngrok

sau ƒë√≥ API s·∫Ω ready ƒë·ªÉ call t·ª´ local!
"""

# =====================================================
# CELL 0: IMPORTS & SETUP BAN ƒê·∫¶U
# =====================================================

import os
import sqlite3
import json
import warnings
import torch
import re
from typing import List
from datetime import datetime

warnings.filterwarnings("ignore")

print("=" * 70)
print("üöÄ HINNE - NUTRITION AI (COLAB SETUP)")
print("=" * 70 + "\n")

# =====================================================
# CELL 1: C√ÄI ƒê·∫∂T DEPENDENCIES
# =====================================================

print("üì¶ STEP 1: C√†i ƒë·∫∑t th∆∞ vi·ªán...\n")

packages = [
    "langchain>=0.2.16",
    "langchain-community>=0.2.16",
    "faiss-cpu",
    "sentence-transformers",
    "transformers",
    "accelerate",
    "bitsandbytes",
    "flask",
    "flask-cors",
    "pyngrok"
]

import subprocess
for pkg in packages:
    subprocess.run(["pip", "install", "-q", pkg], check=False)

print("‚úÖ T·∫•t c·∫£ th∆∞ vi·ªán ƒë√£ c√†i ƒë·∫∑t\n")

# =====================================================
# CELL 2: IMPORT LIBRARIES
# =====================================================

print("üìö STEP 2: Import libraries...\n")

from google.colab import drive
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain.retrievers.document_compressors import CrossEncoderReranker
from langchain.retrievers import ContextualCompressionRetriever
from langchain_community.cross_encoders import HuggingFaceCrossEncoder
from langchain_community.llms import Ollama
from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from flask import Flask, request, jsonify
from flask_cors import CORS
from pyngrok import ngrok

print("‚úÖ Imports th√†nh c√¥ng\n")

# =====================================================
# CELL 3: MOUNT GOOGLE DRIVE & SETUP PATHS
# =====================================================

print("üíæ STEP 3: Mount Google Drive...\n")

drive.mount("/content/drive")

DB_PATH = "/content/drive/MyDrive/test/fitness_data2.db"
FAISS_INDEX_PATH = "/content/faiss_nutrition_index"

EMBED_MODEL = "intfloat/multilingual-e5-base"
EMBED_DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
K_DOCS = 6

USE_OLLAMA = True
OLLAMA_MODEL = "gemma3"
HF_MODEL_ID = "google/gemma-3-12b-it-qat"

print(f"‚úÖ DB_PATH: {DB_PATH}")
print(f"‚úÖ EMBED_DEVICE: {EMBED_DEVICE}")
print(f"‚úÖ USE_OLLAMA: {USE_OLLAMA}\n")

# =====================================================
# CELL 4: LOAD DATA & CREATE VECTOR STORE
# =====================================================

print("üóÇÔ∏è  STEP 4: Load data t·ª´ DB...\n")

conn = sqlite3.connect(DB_PATH)
cursor = conn.cursor()

cursor.execute("""
    SELECT id, food_nameEN, food_nameVN, category,
           calories, protein, carbs, fat, fiber,
           description, usda_id
    FROM fitness_foods
""")
rows = cursor.fetchall()
conn.close()

if not rows:
    raise RuntimeError("‚ùå Database tr·ªëng!")

print(f"‚úÖ ƒê√£ load {len(rows)} th·ª±c ph·∫©m\n")

# =====================================================
# CELL 5: PROCESS TEXT & METADATA
# =====================================================

print("üîÑ STEP 5: X·ª≠ l√Ω text & metadata...\n")

texts, metadatas = [], []

for row in rows:
    (id_, food_en, food_vn, category,
     cal, p, c, f, fi,
     desc, usda_id) = row

    text = (
        f"T√™n: {food_vn} (T√™n ti·∫øng Anh: {food_en}). "
        f"Lo·∫°i: {category}. "
        f"M√¥ t·∫£: {desc}. "
        f"Dinh d∆∞·ª°ng (m·ªói 100g): "
        f"{cal} calories (kcal), "
        f"{p}g protein (ƒë·∫°m), "
        f"{f}g fat (ch·∫•t b√©o), "
        f"{c}g carbohydrates (carbs, tinh b·ªôt), "
        f"{fi}g fiber (ch·∫•t x∆°). "
        f"(USDA ID: {usda_id})"
    )
    texts.append(text)

    metadatas.append({
        "id": id_,
        "name": food_vn,
        "name_en": food_en,
        "category": category,
        "usda_id": usda_id,
        "calories": cal,
        "protein": p,
        "carbs": c,
        "fat": f,
        "fiber": fi,
        "description": desc,
        "primary_goal": f"Cung c·∫•p dinh d∆∞·ª°ng {category}",
        "pro_tips_vn": f"Chia nh·ªè kh·∫©u ph·∫ßn {food_vn} ƒë·ªÉ t·ªëi ∆∞u h√≥a h·∫•p thu",
        "comparison_notes_vn": f"{food_vn} c√≥ h√†m l∆∞·ª£ng {p}g ƒë·∫°m/100g"
    })

print(f"‚úÖ ƒê√£ x·ª≠ l√Ω {len(texts)} vƒÉn b·∫£n\n")

# =====================================================
# CELL 6: CREATE EMBEDDINGS & FAISS INDEX
# =====================================================

print("üß† STEP 6: T·∫°o embeddings & FAISS index...\n")

embeddings = HuggingFaceEmbeddings(
    model_name=EMBED_MODEL,
    model_kwargs={"device": EMBED_DEVICE},
    encode_kwargs={"normalize_embeddings": True},
)

if os.path.exists(FAISS_INDEX_PATH):
    print(f"üì• Load FAISS index t·ª´: {FAISS_INDEX_PATH}")
    vectorstore = FAISS.load_local(
        FAISS_INDEX_PATH,
        embeddings,
        allow_dangerous_deserialization=True
    )
else:
    print("üî® T·∫°o FAISS index m·ªõi...")
    texts_prefixed = [f"passage: {t}" for t in texts]
    vectorstore = FAISS.from_texts(texts_prefixed, embeddings, metadatas=metadatas)
    vectorstore.save_local(FAISS_INDEX_PATH)
    print(f"üíæ L∆∞u index t·∫°i: {FAISS_INDEX_PATH}")

print("‚úÖ Vector store ready\n")

# =====================================================
# CELL 7: SETUP RETRIEVER & RERANKER
# =====================================================

print("üîç STEP 7: Setup retriever & reranker...\n")

base_retriever = vectorstore.as_retriever(
    search_type="mmr",
    search_kwargs={"k": 20}
)

hf_ce = HuggingFaceCrossEncoder(model_name="cross-encoder/ms-marco-MiniLM-L-6-v2")
reranker = CrossEncoderReranker(model=hf_ce, top_n=K_DOCS)

compression_retriever = ContextualCompressionRetriever(
    base_retriever=base_retriever,
    base_compressor=reranker
)

print("‚úÖ Retriever & reranker ready\n")

# =====================================================
# CELL 8: SETUP LLM
# =====================================================

print("ü§ñ STEP 8: Setup LLM...\n")

if USE_OLLAMA:
    print("   D√πng Ollama backend (gemma3)")
    llm = Ollama(model=OLLAMA_MODEL, temperature=0)
else:
    print("   D√πng HuggingFace backend")
    def build_hf_llm(model_id=HF_MODEL_ID, temperature=0, max_new_tokens=512):
        tok = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
        kwargs = dict(
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto"
        )
        if torch.cuda.is_available():
            kwargs["load_in_4bit"] = True
        mdl = AutoModelForCausalLM.from_pretrained(model_id, **kwargs)
        gen = pipeline(
            "text-generation",
            model=mdl,
            tokenizer=tok,
            max_new_tokens=max_new_tokens,
            do_sample=False,
            temperature=temperature,
            pad_token_id=tok.eos_token_id
        )
        return HuggingFacePipeline(pipeline=gen)
    llm = build_hf_llm()

print("‚úÖ LLM ready\n")

# =====================================================
# CELL 9: DEFINE HELPER FUNCTIONS
# =====================================================

print("üõ†Ô∏è  STEP 9: Define helper functions...\n")

def clean_output(text: str) -> str:
    """L√†m s·∫°ch output"""
    text = re.sub(r'[üêüüêÆüêìüí¶üí™ü§îüòä]', '', text)
    text = text.replace('```', '').strip()
    greetings = ['Ch√†o!', 'Ch√†o b·∫°n!', 'C·∫£m ∆°n', 'Ch√∫c b·∫°n']
    for g in greetings:
        text = text.replace(g, '')
    if "Final Answer:" in text:
        text = text.split("Final Answer:")[-1].strip()
    return text.strip()

def format_docs(docs):
    """Format documents ƒë·ªÉ hi·ªÉn th·ªã"""
    lines = []
    for i, d in enumerate(docs, 1):
        m = d.metadata
        goal = m.get('primary_goal', 'Ch∆∞a r√µ')
        tips = m.get('pro_tips_vn', 'Ch∆∞a c√≥ m·∫πo')
        comp = m.get('comparison_notes_vn', 'Ch∆∞a c√≥ so s√°nh')

        lines.append(
            f"- #{i} | {m.get('name')} (EN: {m.get('name_en')}) | Lo·∫°i: {m.get('category')}\n"
            f"  Dinh d∆∞·ª°ng/100g: {m.get('calories')} kcal; {m.get('protein')}g ƒë·∫°m; {m.get('carbs')}g carb; {m.get('fat')}g b√©o.\n"
            f"  M·ª•c ti√™u: {goal}\n"
            f"  M·∫πo: {tips}\n"
            f"  So s√°nh: {comp}"
        )
    return "\n".join(lines)

def add_e5_query_prefix(q: str) -> str:
    """Add E5 query prefix"""
    return "query: " + q

print("‚úÖ Helper functions ready\n")

# =====================================================
# CELL 10: DEFINE smart_ask()
# =====================================================

print("üß† STEP 10: Define smart_ask()...\n")

def smart_ask(query: str):
    """Main query function with routing"""
    query_lower = query.lower()

    NUTRITION_KEYWORDS = [
        'calo', 'ƒë·∫°m', 'protein', 'b√©o', 'fat', 'carb', 'carbohydrate',
        'ch·∫•t x∆°', 'fiber', 'bao nhi√™u', 'm·∫•y gam', 'th√†nh ph·∫ßn',
        'g√†', 'b√≤', 'c√°', 't√°o', 'chu·ªëi', 'rau', 'th·ªãt', 'tr·ª©ng'
    ]

    GREETING_KEYWORDS = ['ch√†o', 'hi', 'hello', 'b·∫°n l√† ai', 't√™n g√¨']

    if any(word in query_lower for word in NUTRITION_KEYWORDS):
        try:
            docs = compression_retriever.get_relevant_documents(
                add_e5_query_prefix(query)
            )

            if not docs:
                return "Xin l·ªói, t√¥i kh√¥ng c√≥ th√¥ng tin v·ªÅ th·ª±c ph·∫©m n√†y."

            context = format_docs(docs)

            fallback_prompt_template = f"""
B·∫°n l√† tr·ª£ l√Ω AI Hinne, m·ªôt chuy√™n gia dinh d∆∞·ª°ng th·ªÉ h√¨nh.
Ch·ªâ d·ª±a v√†o th√¥ng tin d∆∞·ªõi ƒë√¢y ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi.

# TH√îNG TIN TRA C·ª®U (dinh d∆∞·ª°ng m·ªói 100g):
{context}

# C√ÇU H·ªéI:
{query}

# QUY T·∫ÆC B·∫ÆT BU·ªòC:
1. LU√îN d√πng Ti·∫øng Vi·ªát.
2. Tr·∫£ l·ªùi c√¢u h·ªèi ch√≠nh.
3. SAU ƒê√ì, d√πng "M·∫πo" v√† "So s√°nh" ƒë·ªÉ gi·∫£i th√≠ch.
4. KH√îNG ƒê∆Ø·ª¢C b·ªãa s·ªë li·ªáu.

# TR·∫¢ L·ªúI:
"""

            answer = llm.invoke(fallback_prompt_template).strip()
            return clean_output(answer)

        except Exception as e:
            return f"L·ªói: {e}"

    elif any(word in query_lower for word in GREETING_KEYWORDS):
        return "Ch√†o b·∫°n! T√¥i l√† Hinne, tr·ª£ l√Ω dinh d∆∞·ª°ng AI."

    else:
        try:
            docs = compression_retriever.get_relevant_documents(
                add_e5_query_prefix(query)
            )
            if not docs:
                return "T√¥i kh√¥ng hi·ªÉu c√¢u h·ªèi. H·ªèi t√¥i v·ªÅ dinh d∆∞·ª°ng th·ª±c ph·∫©m?"

            context = format_docs(docs)
            fallback_prompt_template = f"D·ª±a v√†o: {context}.\n\nC√¢u h·ªèi: {query}"
            answer = llm.invoke(fallback_prompt_template).strip()
            return clean_output(answer)

        except Exception as e:
            return f"L·ªói: {e}"

print("‚úÖ smart_ask() ready\n")

# =====================================================
# CELL 11: SETUP FLASK API + NGROK
# =====================================================

print("üöÄ STEP 11: Setup Flask API + Ngrok...\n")

app = Flask(__name__)
CORS(app)

@app.route('/ask', methods=['POST'])
def ask_endpoint():
    try:
        data = request.get_json()
        query = data.get('query', '').strip()

        if not query:
            return jsonify({"success": False, "error": "Query tr·ªëng"}), 400

        print(f"[{datetime.now().strftime('%H:%M:%S')}] üì• {query[:80]}")
        answer = smart_ask(query)
        print(f"[{datetime.now().strftime('%H:%M:%S')}] ‚úÖ Tr·∫£ l·ªùi\n")

        return jsonify({"success": True, "query": query, "answer": answer})

    except Exception as e:
        print(f"[ERROR] {e}\n")
        return jsonify({"success": False, "error": str(e)}), 500

@app.route('/health', methods=['GET'])
def health():
    return jsonify({"status": "alive", "message": "‚úÖ API s·∫µn s√†ng"})

@app.route('/info', methods=['GET'])
def info():
    return jsonify({
        "name": "Hinne - Nutrition AI",
        "version": "1.0",
        "endpoints": ["POST /ask", "GET /health", "GET /info"]
    })

print("‚úÖ Flask app created\n")

# =====================================================
# CELL 12: CREATE NGROK TUNNEL
# =====================================================

print("üì° STEP 12: Create Ngrok tunnel...\n")

# SET AUTH TOKEN HERE!
AUTH_TOKEN = "2WuXKz8T_YOUR_TOKEN_HERE"  # ‚Üê UPDATE THIS!

if AUTH_TOKEN == "2WuXKz8T_YOUR_TOKEN_HERE":
    print("‚ö†Ô∏è  CH∆ØA C√ÄI NGROK AUTH TOKEN!")
    print("    1. Truy c·∫≠p: https://dashboard.ngrok.com/auth")
    print("    2. Copy token")
    print("    3. S·ª≠a d√≤ng: AUTH_TOKEN = \"YOUR_TOKEN\"")
    print()
else:
    ngrok.set_auth_token(AUTH_TOKEN)
    print("‚úÖ Ngrok auth token set\n")

public_url = ngrok.connect(5000)

print("=" * 70)
print("‚úÖ NGROK TUNNEL READY!")
print("=" * 70)
print(f"\nüåê PUBLIC URL: {public_url}\n")
print(f"üìù ENDPOINTS:\n")
print(f"   POST {public_url}/ask - G·ª≠i c√¢u h·ªèi")
print(f"   GET  {public_url}/health - Ki·ªÉm tra")
print(f"   GET  {public_url}/info - Th√¥ng tin\n")
print(f"üí° NEXT STEPS:\n")
print(f"   1. Copy URL tr√™n\n")
print(f"   2. M·ªü streamlit_chatbot.py tr√™n local\n")
print(f"   3. S·ª≠a: API_URL = \"{public_url}\"\n")
print(f"   4. Ch·∫°y: streamlit run streamlit_chatbot.py\n")
print("=" * 70 + "\n")

# =====================================================
# CELL 13: RUN FLASK SERVER
# =====================================================

print("üîÑ STEP 13: Start Flask server at 0.0.0.0:5000...\n")
print("üì® Server ƒëang ch·ªù request t·ª´ local client...\n")
print("‚ö†Ô∏è  ƒê·ª™ T·∫ÆT CELL N√ÄY! N√≥ s·∫Ω ch·∫°y li√™n t·ª•c.\n")
print("=" * 70 + "\n")

# Run server (blocking)
app.run(host='0.0.0.0', port=5000, debug=False, use_reloader=False)
